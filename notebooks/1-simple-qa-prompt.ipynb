{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. RAG Workshop: Introduction\n",
    "\n",
    "In This workshop we will use notebooks and python scripts to interactively learn about Large Language Models and RAGs.\n",
    "\n",
    "Large language models are machine learning models that can generate human-like text. They were trained on large amounts of data and can be used to generate text, answer questions, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with jupyter notebook\n",
    "First of all lets make sure you understand the jupyter notebook interface.\n",
    "In jupter you can have cells of text or code.\n",
    "You can type any python code in a cell and press shift + enter to run it.\n",
    "\n",
    "Interact with the cell below and run it multiple times to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-11-10 17:46:55'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = a if a is not None else 2\n",
    "a = a + 2 \n",
    "print(a)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralAI\n",
    "\n",
    "In whis workshop we will use MistralAI models, its the same concept as OpenAI (chatgpt) or Anthropic (claude).\n",
    "\n",
    "One can use mistal on your own computer or use the cloud version.\n",
    "For the sake of this workshop we will use the cloud version as we dont need to download big models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "# the model is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I don't have a name, but you can call me Assistant if you'd like. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "def call_mistral_model(message):\n",
    "    response =mistral_client.chat.complete(\n",
    "        model = model_name,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message,\n",
    "            }\n",
    "            ]\n",
    "        )\n",
    "    # extract only the text from the response\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text\n",
    "\n",
    "print(call_mistral_model(\"hello! What is your name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Simple Q&A RAG\n",
    "\n",
    "Large language models (LLMs) can sometimes hallucinate, presenting false information due to outdated training data. Retrieval-Augmented Generation (RAG) allows us to incorporate external information to mitigate these challenges. In this task, we will create a simple Q&A RAG that utilizes knowledge from a PDF to enrich its answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the text, we can begin enriching the prompt to make our LLM even smarter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message, context):\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can compare how our LLM differs the answers by the information that you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " I'm an assistant that operates solely on the data it has been trained on up until 2021, and I don't have real-time or future weather data. Therefore, I can't provide information on the weather in Berlin on December 10, 2027. To find out the weather forecast for a specific date, I would recommend checking a reliable weather website or application closer to the desired date.\n",
      "----------\n",
      "RAG RESPONSE:\n",
      " The weather in Berlin on the 10th of December, 2027, is expected to be 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "The weather in Berlin on the 10th of December of 2027 will be 10 degrees Celsius.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compare_llm_answers(message):\n",
    "    generic_response = call_mistral_model(message)\n",
    "    \n",
    "    rag_prompt = create_rag_prompt(message=message, context=text)\n",
    "    rag_response = call_mistral_model(rag_prompt)\n",
    "\n",
    "    print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"RAG RESPONSE:\\n {rag_response}\")\n",
    "\n",
    "compare_llm_answers(\"What will be the weather in Berlin on the 10th of December of 2027?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, not just PDFs, such as Google search results, social media posts, and more. With that, weâ€™ve built a simple Q&A RAG. In the next chapter, we will scale it up to include even more context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
