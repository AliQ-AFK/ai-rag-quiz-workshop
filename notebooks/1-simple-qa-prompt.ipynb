{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and RAG: Introduction\n",
    "\n",
    "In this workshop we will use notebooks and Python scripts to interactively learn about Large Language Models and RAG.\n",
    "\n",
    "Large Language Models (LLMs) are a type of machine learning models designed to understand and generate human language. They are trained on massive datasets of text to predict and generate language based on given prompts, learning patterns, structures, and relationships in text to produce human-like responses. They can be used to generate text, answer questions, and more.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines language generation with real-time data retrieval, allowing models to access external sources or databases to provide more accurate, contextually relevant answers.\n",
    "\n",
    "RAG combines two main components:\n",
    "- Retrieval: This component searches and retrieves relevant information from external databases or documents.\n",
    "- Generation: This component uses the retrieved information to generate more accurate and contextually relevant responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Jupyter notebook\n",
    "First of all, let's make sure you understand the Jupyter notebook interface.\n",
    "In Jupter you can have cells of either text or code.\n",
    "You can type any python code in a cell and press shift + enter to run it.\n",
    "\n",
    "Interact with the cell below and run it multiple times to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "a = 42 if a is None else a\n",
    "a = a + 2 \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also import libraries and use them in the cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-11-17 09:44:00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralAI\n",
    "\n",
    "In this workshop, we will be using MistralAI's LLMs, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude.\n",
    "\n",
    "To start working with Mistral, you first need to install the library. We did that for you already.\n",
    "\n",
    "```bash\n",
    "pip install mistralai\n",
    "```\n",
    "\n",
    "\n",
    "The second  step is to get a Mistral api key. You can find some APIs keys we prepared for this workshop in this [doc](https://docs.google.com/document/d/1IjC8TbvxWCoZ0dAKyaTKSdnoQg12kn0D-OWkm1f6i4Y/edit?tab=t.0). Get the key and write it to the .env file using the command\n",
    "\n",
    "```\n",
    "echo 'MISTRAL_API_KEY=\"your_api_key_here\"' >> .env\n",
    "```\n",
    "\n",
    "You can run LLMs on your local machine or use the cloud version.\n",
    "For the sake of this workshop we will use the cloud version as we dont need to download big models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the code below to import Mistral and initialize the Mistral client: <br> _(Note: If you run into a ModuleNotFoundError when trying to run the code below, run the command pip install -r requirements.txt in your terminal and try again after it finishes)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral_model(message):\n",
    "    response = mistral_client.chat.complete(\n",
    "        model = model_name,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message,\n",
    "            }\n",
    "            ]\n",
    "        )\n",
    "    # Extract only the text from the response\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out\n",
    "response = call_mistral_model(\"hello! What is your name?\")\n",
    "\n",
    "# Print the response from the Mistral model\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Rate Limiting\n",
    "\n",
    "When using APIs, you might encounter rate limiting. Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse and ensure fair use of the API resources. \n",
    "\n",
    "However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue. \n",
    "\n",
    "You don't need to understand all the code in the class, but feel free to have a look in chat_solution.llm in case you're curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "# This class includes logic to counteract rate limiting by adding appropriate sleep intervals between requests\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "# Print the response from the Mistral model\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Large language models (LLMs) can sometimes hallucinate, presenting false information due to outdated training data. Retrieval-Augmented Generation (RAG) allows us to incorporate external information to mitigate these challenges. In this task, we will create a simple Q&A RAG that utilizes knowledge from a PDF to enrich its answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the text, we can begin enriching the prompt to make our LLM even smarter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can compare how our LLM differs the answers by the information that you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'call_mistral_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mThe weather in Berlin  December of 2027 will be around 13 degrees Celsius.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mSpecific dates:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m- 20th of December: 7 degrees Celsius\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m message\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat will be the weather in Berlin on the 10th of December of 2027?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m generic_response \u001b[38;5;241m=\u001b[39m \u001b[43mcall_mistral_model\u001b[49m(message)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGENERIC RESPONSE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneric_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m rag_prompt \u001b[38;5;241m=\u001b[39m create_rag_prompt(message\u001b[38;5;241m=\u001b[39mmessage, context\u001b[38;5;241m=\u001b[39mcontext)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'call_mistral_model' is not defined"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "message= \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = call_mistral_model(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = call_mistral_model(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, not just PDFs, such as Google search results, social media posts, and more. With that, we’ve built a simple Q&A RAG. In the next chapter, we will scale it up to include even more context.\n",
    "In the next notebook you fill find how to prepare data for RAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
