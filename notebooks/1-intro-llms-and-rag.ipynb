{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and Retrival Augmented Geenration (RAG): Introduction\n",
    "\n",
    "Welcome to this AI workshop! In this workshop we will get deeper into AI, RAG, chatbots, embeddings, evaluations and more.\n",
    "But first lets get used to this notebook enviroment.\n",
    "\n",
    "### Getting started with Jupyter Notebooks\n",
    "Notebooks are one of the most imporant tools in teh toolbelt of a data scientist of ML engineer.\n",
    "They are great for interacting with code and visualizing results.\n",
    "This file you are interacting with is a jupyter notebook. In Jupter you can have cells of either text or code.\n",
    "Cells with code can be run by pressing ctrl+enter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Run the cell bellow a couple of times to get used to the notebook behaviour.\n",
    "2. Restart the kernel (Restart) button above and run again this code to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a if it does not exist\n",
    "if  'a' not in globals():\n",
    "    a = 42\n",
    "\n",
    "a = a+1\n",
    "\n",
    "# press ctrl+enter to run the cell, do that multiple times\n",
    "# notice how the variable `a` is persistent in the cell\n",
    "# you can run full programs in a notebook.\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LLMs\n",
    "Large Language Models (LLMs) are a type of machine learning models designed to understand and generate human language. They are trained on massive datasets of text to predict and generate language based on given prompts, learning patterns, structures, and relationships in text to produce human-like responses. They can be used to generate text, answer questions, and more.\n",
    "\n",
    "### MistralAI\n",
    "\n",
    "For this workshop we wil use [Mistral](https://mistral.ai/) langauge models, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude. If your installation finalized correctly you should have mistral already installed. \n",
    "The command below checks if mistral is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mistralai\n",
      "Version: 1.1.0\n",
      "Summary: Python Client SDK for the Mistral AI API.\n",
      "Home-page: https://github.com/mistralai/client-python.git\n",
      "Author: Mistral\n",
      "Author-email: \n",
      "License: \n",
      "Location: /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages\n",
      "Requires: eval-type-backport, httpx, jsonpath-python, pydantic, python-dateutil, typing-inspect\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason mistral did not installed successfully you can get it (and all other dependencies of this workshop by installing it directly with the command below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/explodinggradients/ragas (from -r ../requirements.txt (line 11))\n",
      "  Cloning https://github.com/explodinggradients/ragas to /tmp/pip-req-build-_ol1klm1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/explodinggradients/ragas /tmp/pip-req-build-_ol1klm1\n",
      "  Resolved https://github.com/explodinggradients/ragas to commit 7d051437a1a5d8e9ad5c42252bf1debf51679140\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers~=3.2.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: pandas~=2.2.3 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: mistralai~=1.1.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: watchdog~=5.0.3 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (5.0.3)\n",
      "Requirement already satisfied: PyPDF2~=3.0.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: streamlit~=1.39.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (1.39.0)\n",
      "Requirement already satisfied: python-dotenv in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: ipykernel in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (6.29.5)\n",
      "Requirement already satisfied: fire in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: langchain_mistralai in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: langchain-huggingface in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: nltk in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 13)) (3.9.1)\n",
      "Requirement already satisfied: rapidfuzz in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 14)) (3.10.1)\n",
      "Requirement already satisfied: matplotlib in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 15)) (3.9.2)\n",
      "Requirement already satisfied: einops in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 16)) (0.8.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: scipy in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.27.2)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.0.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (2.10.1)\n",
      "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas~=2.2.3->-r ../requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (5.4.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: packaging<25,>=20 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (5.28.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (18.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (6.4.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (1.8.9)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (8.29.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (26.2.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (5.14.3)\n",
      "Requirement already satisfied: termcolor in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from fire->-r ../requirements.txt (line 9)) (2.5.0)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain_mistralai->-r ../requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain_mistralai->-r ../requirements.txt (line 10)) (0.3.20)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain_mistralai->-r ../requirements.txt (line 10)) (0.20.3)\n",
      "Requirement already satisfied: datasets in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (3.1.0)\n",
      "Requirement already satisfied: tiktoken in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.8.0)\n",
      "Requirement already satisfied: langchain in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.3.7)\n",
      "Requirement already satisfied: langchain-community in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.3.7)\n",
      "Requirement already satisfied: langchain_openai in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.2.9)\n",
      "Requirement already satisfied: appdirs in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: openai>1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (1.55.0)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.3.4)\n",
      "Requirement already satisfied: joblib in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from nltk->-r ../requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from nltk->-r ../requirements.txt (line 13)) (2024.11.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 15)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 15)) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 15)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from matplotlib->-r ../requirements.txt (line 15)) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.5.2 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (4.0.11)\n",
      "Requirement already satisfied: anyio in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: idna in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: sniffio in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: filelock in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: decorator in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r ../requirements.txt (line 8)) (4.3.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10)) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10)) (0.1.145)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from openai>1->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from openai>1->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: networkx in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (3.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (2.0.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.3.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-community->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langchain-community->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (2.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from scikit-learn->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (1.18.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (3.23.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (0.21.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10)) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.2.13)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11)) (3.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /workspaces/rag-workshop/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# this command will install all the dependencies of the requirements.txt file\n",
    "!pip install -r ../requirements.txt\n",
    "\n",
    "# this command will install the code of the project itsel\n",
    "!pip install -e ../\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting access to Mistral LLMs\n",
    "\n",
    "LLMs are usually big and use a lot of computer resources the most common way to use them is to run them in teh cloud via network calls. For the sake of this workshop we will use the cloud version as well so we dont need to download big models.\n",
    "If you are interested in running llms locally check [ollama](https://github.com/ollama/ollama), arguably the most advanced project that does it.\n",
    "\n",
    "\n",
    "#### API key\n",
    "The second  step is to get a Mistral api key. You can find some APIs keys we prepared for this workshop in this [sheet](https://docs.google.com/spreadsheets/d/1ZwTpkG6OOuVrOx8nzPmgai_7Hwpo8Kun7yZrOmg_5K4/edit?gid=0#gid=0). Get the key (please write your name next to it in the sheet such that people know it is taken) and write it to the .env file using the command\n",
    "\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Use the cell below to write your api key into the file so you can use mistral.\n",
    "We write the variable into a file named .env. .env files a are a standard in python to load information like api keys and passwords that should not stay with the code for security reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file .env\n",
    "\n",
    "mistral_api_key = 'REPLACE WITH YOUR KEY'\n",
    "with open('../.env', 'w') as f:\n",
    "    f.write(f'MISTRAL_API_KEY=\"{mistral_api_key}\"')\n",
    "\n",
    "# make sure to delete teh API key from this cell before commiting the file so you dont save your key in the repo which is a security risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if it worked\n",
    "\n",
    "Run the command below to check if writing the key worked.  It will trigger an error if it did not.  You might need to restart the kernel if it does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now reload the the configuration file and it should show your key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env_file = find_dotenv()\n",
    "print(f\"Loading environment variables from {env_file}\")\n",
    "load_dotenv(env_file)\n",
    "\n",
    "import os\n",
    "env = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# you should see your key here not\n",
    "print(\"The key is: \", env)\n",
    "\n",
    "if not env:\n",
    "    raise ValueError(\"The API key is not set. Please set the MISTRAL_API_KEY environment variable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mistral\n",
    "Let's run the code below to import Mistral and initialize the Mistral client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\"\n",
    "\n",
    "# The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text.\n",
    "response = mistral_client.chat.complete(\n",
    "    model = model_name,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"hello! What is your name?\",\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "# Extract only the text from the response\n",
    "response_text = response.choices[0].message.content\n",
    "print(response_text)\n",
    "\n",
    "## not how the result resembles natural language. Its the exact same concept as chatgpt.\n",
    "## feel free to play around with the prompt and see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having all of this complex code to call the llm, going forward, we can move this complexity to a python script.\n",
    "This is what we are doing from the code below on. We are replacing our calls to mistral api to calls to LargeLanguageModel the language model class.\n",
    "\n",
    "Try it out, but also take a look into the class..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /Users/jean.machado@getyourguide.com/prj/rag-workshop/.env\n",
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond calling the llm, the class contain rate limiting handling logic.  Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse so 1 player dont make the whole capacity used and slow.  However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a rate limit error controller in LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue. \n",
    "\n",
    "Have a look into the class by ctrl+ clicking in the **LargeLanguageModel* name to jump directly to its implementation.\n",
    "\n",
    "## Exploring the LLM\n",
    "Now that we have seen how to make a basic call to the Mistral model using the [LargeLanguageModel](../chat_solution/llm.py) class, let's try some more prompts to see how the model responds to different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Modifying Prompts\n",
    "Below are some example use cases of how to use an LLM such as Mistral. Play around with the prompts and see the results. Modify the prompts to see how the model's responses change. This will help you understand how to craft effective prompts and get the desired output from the model.\n",
    "\n",
    "Try to:\n",
    "- Ask different types of questions\n",
    "- Change the text for summarization or extraction (see examples 2 and 3 below)\n",
    "- Alter the style of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Asking for Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! 42Berlin is a coding school that follows the innovative educational model pioneered by 42 Network, which originated in France. Here are some key aspects of 42Berlin:\n",
      "\n",
      "### Educational Model\n",
      "1. **Peer-to-Peer Learning**: 42Berlin emphasizes a peer-to-peer learning environment where students learn from and teach each other. This model encourages collaboration and mutual support.\n",
      "\n",
      "2. **Project-Based Learning**: The curriculum is heavily focused on practical projects. Students work on real-world projects that help them develop both technical skills and problem-solving abilities.\n",
      "\n",
      "3. **No Teachers or Classes**: Unlike traditional schools, 42Berlin does not have teachers or formal classes. Instead, students learn through a combination of online resources, projects, and peer interactions.\n",
      "\n",
      "4. **24/7 Access**: The school is open 24 hours a day, 7 days a week, allowing students to work at their own pace and on their own schedule.\n",
      "\n",
      "### Admission Process\n",
      "1. **Piscine (Pool)**: The admission process begins with a four-week intensive bootcamp called \"Piscine.\" During this period, applicants are evaluated on their problem-solving skills, motivation, and ability to work in a team.\n",
      "\n",
      "2. **No Prerequisites**: 42Berlin does not require any formal education or prior coding experience. The Piscine is designed to assess potential rather than prior knowledge.\n",
      "\n",
      "### Curriculum\n",
      "1. **Core Programming Skills**: Students learn fundamental programming concepts and languages such as C, C++, and eventually move on to more advanced topics like web development, data structures, and algorithms.\n",
      "\n",
      "2. **Soft Skills**: In addition to technical skills, the program emphasizes the development of soft skills such as communication, teamwork, and project management.\n",
      "\n",
      "3. **Flexible Paths**: The curriculum is designed to be flexible, allowing students to explore different areas of interest within computer science.\n",
      "\n",
      "### Community and Support\n",
      "1. **Global Network**: As part of the 42 Network, 42Berlin students have access to a global community of peers, alumni, and industry professionals.\n",
      "\n",
      "2. **Mentorship**: While there are no traditional teachers, experienced students and alumni often mentor new students, providing guidance and support.\n",
      "\n",
      "### Outcomes\n",
      "1. **Job Placement**: 42Berlin has a strong focus on preparing students for careers in the tech industry. Many graduates go on to work for top tech companies.\n",
      "\n",
      "2. **Entrepreneurship**: The school also encourages entrepreneurship, and some graduates go on to start their own tech ventures.\n",
      "\n",
      "### Location\n",
      "42Berlin is located in Berlin, Germany, a city known for its vibrant tech scene and startup culture. This location provides students with access to numerous networking opportunities and potential job prospects.\n",
      "\n",
      "Overall, 42Berlin offers a unique and innovative approach to learning programming and computer science, focusing on practical skills, peer collaboration, and real-world projects.\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking for information\n",
    "prompt = \"Can you tell me about coding school 42Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42Berlin is a tuition-free coding school empowering the next generation of coders through accessible and inclusive tech education.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_summarize = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Summarize the following text in one brief sentence: {text_to_summarize}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Extracting Information from a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year 42Berlin was founded is 2021.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_extract_from = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Extract the year 42Berlin was founded from the following text: {text_to_extract_from}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3: Doing maths -> When will the LLM make a mistake?\n",
    "\n",
    "LLMs are trained on text language so they are not necessarily good with maths. We can be confident that it will fail to produce the correct result if you ask a question that is complex enough. On the notebook below we increase the complexity of the task at every run. Run it until you see it diverging.\n",
    "\n",
    "What was te number of iterations necessary to make it break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "WARNING:root:Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}Waiting {time_to_wait} seconds before retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct result:  4\n",
      "Mistral result:  The result of 2 ** 2 is 4. In Python, the `**` operator is used for exponentiation, so 2 raised to the power of 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "if 'a' not in globals() or 'b' not in globals():\n",
    "    a = 1\n",
    "    b = 1\n",
    "a = a + 1\n",
    "b = b + 1\n",
    "\n",
    "## ** in python is the power operator meaning: a to the power of b\n",
    "print(\"Correct result: \", a**b)\n",
    "response = llm.call(f\"what is the results of {a} ** {b}\")\n",
    "print(\"Mistral result: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably noted, mistral does not only return the results, it also explains it step by step. For example:\n",
    "\n",
    "```\n",
    "The result of 4 raised to the power of 4 (4 ** 4) is calculated as:\n",
    "4 * 4 * 4 * 4 = 256\n",
    "```\n",
    "\n",
    "Mistral and other language models deliberatelly add this longer explanations as they help the model commit less mistakes. This pattern is also known as chain of thought reasonsing. And is one of the most robust ways to improve LLM's responses. When you write prompts think about how to explain the steps of the reasonsing to improve the performance of your propmts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "LLMs sometimes generate responses that are plausible-sounding but factually incorrect or nonsensical. This phenomenon is known as \"hallucination\". \n",
    "Hallucination can occur because the model generates text based on patterns in the training data rather than actual knowledge or retrieval of relevant information.\n",
    "LLMs will produce the most likelly words that they would find in similar text, they have no clue about fact vs fiction. They can confidently produce writing about things they have no clue about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Demonstrating Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the model a question that it might to hallucinate an answer for, showing the limitations of relying solely on language generation without retrieval.\n",
    "\n",
    "Try running the command below a few times in a row and see how the response by the LLM changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response likely to hallucinate:\n",
      "\n",
      "The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Beginners.\" This workshop aimed to introduce participants to the fundamentals of MLOps (Machine Learning Operations) and provide them with practical skills and knowledge to implement and manage machine learning projects effectively.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question likely to cause hallucination\n",
    "prompt = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "response = llm.call(prompt)\n",
    "print(\"Response likely to hallucinate:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to the next section on Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that incorporates external information to a LLM to generate better responses. \n",
    "\n",
    "In RAG, a retrieval component searches and retrieves relevant information from a knowledge base or external documents, and a generation component uses this information to generate responses.\n",
    "This approach allows the model to access up-to-date information and provide more detailed and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will demonstrate a simple example of how to use Retrieval-Augmented Generation. We will use a predefined set of documents, retrieve relevant information based on a query, and then generate a response using the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code in the cell below, you can compare how our LLM responses differ by the information that you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Beginners.\" This workshop aimed to introduce participants to the fundamentals of MLOps (Machine Learning Operations) and provide them with practical skills and knowledge to implement MLOps in their projects.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " The name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech is \"AI Launchpad - Building Your First ML Pipeline.\"\n"
     ]
    }
   ],
   "source": [
    "message = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "# The workshop the MLOps Community hosted together with Girls in Tech Germany was called \"AI Launchpad: Building Your First Ml Pipeline\" or simply \"Building Your First ML Pipeline\"\n",
    "# We copy paste the info from our Eventbrite event page from the previous workshop and use this as context for the model to retrieve the right info from\n",
    "context = \"\"\"Title: AI Launchpad - Building Your First ML Pipeline: \n",
    "On Wednesday, June 5th, 2024, the MLOps Community Berlin in collaboration with Girls in Tech Germany hosted an interactive workshop for beginners who want to kick start their career in AI/ML. \n",
    "The workshop starts at 18.00h at 42Berlin. \n",
    "\n",
    "🔍 Why Attend?\n",
    "\n",
    "Gain hands-on experience building your first ML pipeline in an agile way\n",
    "Apply the fundamentals of statistical modeling and basic Python\n",
    "Opportunities to improve your portfolio \n",
    "Connect with ML professionals at different levels of seniority\n",
    "\n",
    "\n",
    "✨ The Agenda: \n",
    "\n",
    "6:00 pm - Arrive & Pizza \n",
    "6:30 pm -  Introduction MLOps and GiT\n",
    "6:45 pm - Workshop Introduction\n",
    "7:30 pm - Break\n",
    "7:45 pm - Workshop\n",
    "9:45 pm - Networking\n",
    "\n",
    "\n",
    "🎉 Highlights:\n",
    "\n",
    "Food and drinks provided\n",
    "Engaging discussions and networking opportunities\n",
    "Bring your laptop and get ready to learn!\n",
    "\n",
    "\n",
    "💼 Who Should Attend?\n",
    "\n",
    "Individuals starting their career in Machine Learning or Artificial Intelligence\n",
    "Those looking to transition into the field of AI/ML\n",
    "Anyone interested in contributing to and learning from the ML community\n",
    "Don't miss out on this chance to gain practical AI/ML skills while expanding your professional network! \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "WARNING:root:Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}Waiting {time_to_wait} seconds before retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " I'm an assistant that operates solely on the data it has been trained on up until 2021, and I don't have real-time or future weather data. Therefore, I can't provide the weather forecast for Berlin on the 10th of December, 2027. For the most accurate and up-to-date weather information, I recommend checking a reliable weather website or application closer to the date.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " On the 10th of December, 2027, the weather in Berlin is expected to be around 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same thing with Berlin weather data!\n",
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "\n",
    "message = \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Now try it yourself! Can you find some content on the internet (think, for example, recent news articles or very specific, locally relevant information that the LLM normally would not have access to). \n",
    "\n",
    "Play around with it and let the creative juices flow. Can you discover some more use cases for which you can use RAG can help make our LLM smarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "WARNING:root:Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}Waiting {time_to_wait} seconds before retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " Hello! How can I assist you today? Let me know if you have any questions or need help with something.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " Sure, please provide the context and the user's question.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"\"\"\n",
    "YOUR CONTEXT HERE\n",
    "\"\"\" \n",
    "\n",
    "message = \"\"\"\n",
    "YOUR MESSAGE HERE\n",
    "\"\"\" \n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, such as PDFs, Google search results, social media posts, and more. With that, we’ve built a simple Q&A RAG.\n",
    "\n",
    "## ===> Now head to notebook 2 on embedddings and how llms undertand language\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".virtualenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
