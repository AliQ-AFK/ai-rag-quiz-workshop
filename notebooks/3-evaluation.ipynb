{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c107c1db-f471-4ccc-8fad-e6fbe08d0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /Users/jean.machado@getyourguide.com/prj/rag-workshop/.env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 chunks of size 700 with overlap 200\n",
      "Loading database from /tmp/embedding_db.pkl\n",
      "Database saved to /tmp/embedding_db.pkl\n",
      "Database saved successfully\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.create_db import create_db\n",
    "\n",
    "db = create_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27daf9d1-34ee-4a79-9af5-b556aadc6958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rs to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embeddings of prompts rather than the prompt text itself to improve model output. It is \\n\\nWhat is Fine-tunning?\\nTBD\\n\\n What are multimodal models? \\nMultimodal models can process and integrate multiple types of data, such as text, images, and audio, to create richer and more comprehensive outputs across different input types.\\n\\nEvaluation\\nEvaluation in LLMs measures model performance using metrics such as accuracy, relevance, and coherence, assessing how well the model fulfills its intended purpose and meet', 'rs to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embeddings of prompts rather than the prompt text itself to improve model output. It is \\n\\nWhat is Fine-tunning?\\nTBD\\n\\n What are multimodal models? \\nMultimodal models can process and integrate multiple types of data, such as text, images, and audio, to create richer and more comprehensive outputs across different input types.\\n\\nEvaluation\\nEvaluation in LLMs measures model performance using metrics such as accuracy, relevance, and coherence, assessing how well the model fulfills its intended purpose and meet', 'e LLMs are proprietary, with code and models kept private, while open-source LLMs allow public access to the model architecture and often the training data, enabling more transparency and community-driven improvements.\\n\\n\\n\\nWhat is RAG?\\n\\nRetrieval-Augmented Generation (RAG) combines language generation with real-time data retrieval, allowing models to access external sources or databases to provide more accurate, contextually relevant answers.\\n\\n\\nWhat is a hallucination?\\n A hallucination in AI refers to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embedding', 'e LLMs are proprietary, with code and models kept private, while open-source LLMs allow public access to the model architecture and often the training data, enabling more transparency and community-driven improvements.\\n\\n\\n\\nWhat is RAG?\\n\\nRetrieval-Augmented Generation (RAG) combines language generation with real-time data retrieval, allowing models to access external sources or databases to provide more accurate, contextually relevant answers.\\n\\n\\nWhat is a hallucination?\\n A hallucination in AI refers to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embedding', 'ent input types.\\n\\nEvaluation\\nEvaluation in LLMs measures model performance using metrics such as accuracy, relevance, and coherence, assessing how well the model fulfills its intended purpose and meets user needs.\\nLLM-based applications require systematic testing and evaluation due to their complexity. (2)\\n\\nMonitoring \\n\\nMonitoring involves tracking an LLM’s performance in real-time use to ensure accuracy, reliability, and ethical standards are maintained, often incorporating feedback to improve model outputs over time.\\n\\nExamples of applying Gen AI\\nGenerative AI can be used for tasks like customer service automation, content creation, coding assistance, personalized marketing, and even medica']\n"
     ]
    }
   ],
   "source": [
    "print(db.retrieve(\"How do you pick a green?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0362f7-abc1-4043-9ba8-a3648696e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /Users/jean.machado@getyourguide.com/prj/rag-workshop/.env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database from /tmp/embedding_db.pkl\n",
      "Found 5 documents, first 500 characters: ['rs to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embeddings of prompts rather than the prompt text itself to improve model output. It is \\n\\nWhat is Fine-tunning?\\nTBD\\n\\n What are multimodal models? \\nMultimodal models can process and integrate multiple types of data, such as text, images, and audio, to create richer and more comprehensive outputs across different input types.\\n\\nEvaluation\\nEvaluation in LLMs measures model performance using metrics such as accuracy, relevance, and coherence, assessing how well the model fulfills its intended purpose and meet', 'rs to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embeddings of prompts rather than the prompt text itself to improve model output. It is \\n\\nWhat is Fine-tunning?\\nTBD\\n\\n What are multimodal models? \\nMultimodal models can process and integrate multiple types of data, such as text, images, and audio, to create richer and more comprehensive outputs across different input types.\\n\\nEvaluation\\nEvaluation in LLMs measures model performance using metrics such as accuracy, relevance, and coherence, assessing how well the model fulfills its intended purpose and meet', 'e LLMs are proprietary, with code and models kept private, while open-source LLMs allow public access to the model architecture and often the training data, enabling more transparency and community-driven improvements.\\n\\n\\n\\nWhat is RAG?\\n\\nRetrieval-Augmented Generation (RAG) combines language generation with real-time data retrieval, allowing models to access external sources or databases to provide more accurate, contextually relevant answers.\\n\\n\\nWhat is a hallucination?\\n A hallucination in AI refers to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embedding', 'e LLMs are proprietary, with code and models kept private, while open-source LLMs allow public access to the model architecture and often the training data, enabling more transparency and community-driven improvements.\\n\\n\\n\\nWhat is RAG?\\n\\nRetrieval-Augmented Generation (RAG) combines language generation with real-time data retrieval, allowing models to access external sources or databases to provide more accurate, contextually relevant answers.\\n\\n\\nWhat is a hallucination?\\n A hallucination in AI refers to when a model generates information that sounds plausible but is factually incorrect or unsupported by the training data.\\n\\n\\n\\nPrompt engineering\\nPrompt Tuning: Technique of adjusting the embedding', 'ent input types.\\n\\nEvaluation\\nEvaluation in LLMs measures model performance using metrics such as accuracy, relevance, and coherence, assessing how well the model fulfills its intended purpose and meets user needs.\\nLLM-based applications require systematic testing and evaluation due to their complexity. (2)\\n\\nMonitoring \\n\\nMonitoring involves tracking an LLM’s performance in real-time use to ensure accuracy, reliability, and ethical standards are maintained, often incorporating feedback to improve model outputs over time.\\n\\nExamples of applying Gen AI\\nGenerative AI can be used for tasks like customer service automation, content creation, coding assistance, personalized marketing, and even medica']\n",
      "Based on the provided context, there is no information about how to pick a green. Therefore, I cannot answer the question using the context provided.\n"
     ]
    }
   ],
   "source": [
    "# User input and response handling\n",
    "from chat_solution.rag import QuizRag\n",
    "\n",
    "query1 = \"what is up?\"\n",
    "query2 = \"How do you pick a green?\"\n",
    "rag = QuizRag()  \n",
    "response = rag.query(query2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0c60285d-241a-4027-b7f2-3f27a58e5e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "# Convert each text chunk to a LangChain Document\n",
    "from langchain.schema import Document\n",
    "\n",
    "langchain_docs = [\n",
    "    Document(page_content=text, metadata={\"source\": f\"chunk_{i+1}\"})\n",
    "    for i, text in enumerate(text_chunks)\n",
    "]\n",
    "\n",
    "print(len(langchain_docs))\n",
    "# Display documents with metadata\n",
    "#for doc in langchain_docs:\n",
    "#    print(doc.page_content, doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0000fbcc-8f12-4e14-bc70-642937d55ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatMistralAI(model=\"mistral-large-latest\"))\n",
    "metrics = [LLMContextRecall(), FactualCorrectness(), Faithfulness()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "75feb97b-598c-4a2a-aab5-e4fd91b96a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatMistralAI(model=\"mistral-large-latest\"))\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6c1a3fc0-7d01-4dc7-b340-f9b3c6b63fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(langchain_docs, testset_size=10)\n",
    "\n",
    "#testset = generator.generategenerate_with_langchain_docs(langchain_docs[:10], test_size=10, \n",
    "#                                                 distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e1856cab-65a9-4212-95d5-a42725294ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepared questions dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"atitaarora/food_lab_green_qna\", split=\"train\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b5ee129-023b-4942-bb7d-8717baf88432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '{\"question\": \"If the dark green leaves of chicory are removed and discarded, then how might this affect the overall texture and flavor of the salad?\"}',\n",
       " 'reference_contexts': ['ENDIVE AND CHICORY\\nSALAD\\nWITH GRAPEFRUIT,\\nCRANBERRIES, AND FIG AND\\nPUMPKIN SEED VINAIGRETTE\\nSERVES 4\\n1 head chicory, dark green leaves removed and\\ndiscarded, pale white and yellow sections washed, spun\\ndry, and torn into 2-inch pieces\\n2 Belgian endives, bottoms trimmed, separated into'],\n",
       " 'reference_answer': '{\"answer\": \"Removing and discarding the dark green leaves of chicory results in a salad with a milder flavor and a more tender texture, as the pale white and yellow sections are less bitter and more delicate.\"}',\n",
       " 'new_code': 'true',\n",
       " 'node_metadata': {'excerpt_keywords': 'Keywords: Endive, Chicory, Salad, Grapefruit, Cranberries, Fig, Pumpkin Seed, Vinaigrette',\n",
       "  'page_number': 90,\n",
       "  'source_file_name': '/tmp/food_lab_green_chapter.pdf'},\n",
       " 'question_type': 'CONDITIONAL',\n",
       " 'metadata': {'excerpt_keywords': 'Keywords: Endive, Chicory, Salad, Grapefruit, Cranberries, Fig, Pumpkin Seed, Vinaigrette',\n",
       "  'page_number': 90,\n",
       "  'source_file_name': '/tmp/food_lab_green_chapter.pdf'},\n",
       " 'eval_scores': {'correctness': {'failure_reason': None,\n",
       "   'invalid_result': False,\n",
       "   'passing': True,\n",
       "   'score': 5.0},\n",
       "  'faithfulness': {'invalid_result': False, 'passing': True, 'score': 1},\n",
       "  'relevancy': {'invalid_result': False, 'passing': True, 'score': 1}}}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sample question data\n",
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca66ec5-6486-4956-a361-5b0fed11145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ragas import evaluate\n",
    "#results = evaluate(dataset=dataset, metrics=metrics, llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0cfa60f6-ee2d-4cc8-821c-07b7fddd745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparation of Eval dataset for RAGAS (https://docs.ragas.io/en/stable/concepts/components/eval_sample/?h=singleturnsample#example)\n",
    "##for ragas dataset needs to be in the designated format \n",
    "from ragas import EvaluationDataset, SingleTurnSample\n",
    "from ragas.metrics import Faithfulness\n",
    "from datasets import load_dataset\n",
    "from ragas import evaluate\n",
    "import time\n",
    "\n",
    "samples = []\n",
    "eval_size = 5\n",
    "\n",
    "for i in range(eval_size):\n",
    "    entry = dataset[i]\n",
    "    \n",
    "    # Perform the query with a delay to limit to 1 request per second\n",
    "    user_query = entry['query']\n",
    "    response = rag.query(user_query)\n",
    "    \n",
    "    sample = SingleTurnSample(\n",
    "        user_input=user_query,\n",
    "        reference=entry['reference_answer'],\n",
    "        response=response,\n",
    "        retrieved_contexts=db.retrieve(user_query),\n",
    "    )\n",
    "    samples.append(sample)\n",
    "    \n",
    "    # Wait for 1-2 second before proceeding to the next iteration as we are limited by Mistral API\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e2d5fadb-0058-4a87-9b13-bdbf23e5b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  \\\n",
      "0  (user_input, {\"question\": \"If all vegetables a...   \n",
      "1  (user_input, {\"question\": \"How might the combi...   \n",
      "2  (user_input, {\"question\": \"If the dark green l...   \n",
      "3  (user_input, {\"question\": \"What type of leaves...   \n",
      "4  (user_input, {\"question\": \"What ingredient is ...   \n",
      "\n",
      "                                                   1  \\\n",
      "0  (retrieved_contexts, [ want to use plenty ofwa...   \n",
      "1  (retrieved_contexts, [a dry cooking method, th...   \n",
      "2  (retrieved_contexts, [ vigorously before using...   \n",
      "3  (retrieved_contexts, [omatoes, toasted nuts, a...   \n",
      "4  (retrieved_contexts, [aseddressings tend to be...   \n",
      "\n",
      "                            2  \\\n",
      "0  (reference_contexts, None)   \n",
      "1  (reference_contexts, None)   \n",
      "2  (reference_contexts, None)   \n",
      "3  (reference_contexts, None)   \n",
      "4  (reference_contexts, None)   \n",
      "\n",
      "                                                   3                        4  \\\n",
      "0  (response, If all vegetables are not the same ...  (multi_responses, None)   \n",
      "1  (response, \"Beets bring a sweet and earthy fla...  (multi_responses, None)   \n",
      "2  (response, If the dark green leaves of chicory...  (multi_responses, None)   \n",
      "3  (response, \"It's not my place to tell you that.\")  (multi_responses, None)   \n",
      "4             (response, It is out of my pay grade.)  (multi_responses, None)   \n",
      "\n",
      "                                                   5               6  \n",
      "0  (reference, {\"answer\": \"The natural diversity ...  (rubric, None)  \n",
      "1  (reference, {\"answer\": \"The combination of bee...  (rubric, None)  \n",
      "2  (reference, {\"answer\": \"Removing and discardin...  (rubric, None)  \n",
      "3  (reference, {\"answer\": \"Darker green leaves ar...  (rubric, None)  \n",
      "4  (reference, {\"answer\": \"The shake-it-in-a-jar ...  (rubric, None)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(samples)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8cbfefec-a59a-4a29-9cd9-d6372dbe1c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d13c5a335b44acaa741f75f077c68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "## Actual Evaluation\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "from ragas.metrics import NonLLMContextRecall\n",
    "from ragas.metrics import LLMContextRecall\n",
    "from ragas.metrics import Faithfulness\n",
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "eval_dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "faithfulness = Faithfulness()\n",
    "context_precision = LLMContextPrecisionWithReference()\n",
    "context_recall = NonLLMContextRecall()\n",
    "llm_context_recall = LLMContextRecall()\n",
    "answer_relevancy = ResponseRelevancy()\n",
    "\n",
    "eval_results = evaluate(\n",
    "        dataset=eval_dataset,\n",
    "        metrics=[\n",
    "                faithfulness,\n",
    "                answer_relevancy,\n",
    "                #context_recall, #This metric [non_llm_context_recall] that is used requires the following additional columns ['reference_contexts'] to be present in the dataset.\n",
    "                llm_context_recall,\n",
    "                context_precision,\n",
    "        ],\n",
    "       #llm=evaluator_llm\n",
    "       raise_exceptions=False \n",
    "    )\n",
    "#eval_results = evaluate(\n",
    "#    dataset=eval_dataset,\n",
    "#    metrics=[metric],\n",
    "#llm=evaluator_llm\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "66c140c6-443b-4e1f-919e-afb9954d6a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>llm_context_precision_with_reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"question\": \"If all vegetables are trimmed to...</td>\n",
       "      <td>[ want to use plenty ofwater—it retains its te...</td>\n",
       "      <td>If all vegetables are not the same type, they ...</td>\n",
       "      <td>{\"answer\": \"The natural diversity of vegetable...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.92057</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"question\": \"How might the combination of bee...</td>\n",
       "      <td>[a dry cooking method, they barely lose any ju...</td>\n",
       "      <td>\"Beets bring a sweet and earthy flavor, while ...</td>\n",
       "      <td>{\"answer\": \"The combination of beets, olive oi...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.92550</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"question\": \"If the dark green leaves of chic...</td>\n",
       "      <td>[ vigorously before using.KNIFE SKILLS:How to ...</td>\n",
       "      <td>If the dark green leaves of chicory are remove...</td>\n",
       "      <td>{\"answer\": \"Removing and discarding the dark g...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"question\": \"What type of leaves are not reco...</td>\n",
       "      <td>[omatoes, toasted nuts, and herbs?Probably not...</td>\n",
       "      <td>\"It's not my place to tell you that.\"</td>\n",
       "      <td>{\"answer\": \"Darker green leaves are not recomm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"question\": \"What ingredient is not recommend...</td>\n",
       "      <td>[aseddressings tend to be thicker and creamier...</td>\n",
       "      <td>It is out of my pay grade.</td>\n",
       "      <td>{\"answer\": \"The shake-it-in-a-jar method is no...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  {\"question\": \"If all vegetables are trimmed to...   \n",
       "1  {\"question\": \"How might the combination of bee...   \n",
       "2  {\"question\": \"If the dark green leaves of chic...   \n",
       "3  {\"question\": \"What type of leaves are not reco...   \n",
       "4  {\"question\": \"What ingredient is not recommend...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [ want to use plenty ofwater—it retains its te...   \n",
       "1  [a dry cooking method, they barely lose any ju...   \n",
       "2  [ vigorously before using.KNIFE SKILLS:How to ...   \n",
       "3  [omatoes, toasted nuts, and herbs?Probably not...   \n",
       "4  [aseddressings tend to be thicker and creamier...   \n",
       "\n",
       "                                            response  \\\n",
       "0  If all vegetables are not the same type, they ...   \n",
       "1  \"Beets bring a sweet and earthy flavor, while ...   \n",
       "2  If the dark green leaves of chicory are remove...   \n",
       "3              \"It's not my place to tell you that.\"   \n",
       "4                         It is out of my pay grade.   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  {\"answer\": \"The natural diversity of vegetable...      0.500000   \n",
       "1  {\"answer\": \"The combination of beets, olive oi...      0.636364   \n",
       "2  {\"answer\": \"Removing and discarding the dark g...      0.500000   \n",
       "3  {\"answer\": \"Darker green leaves are not recomm...           NaN   \n",
       "4  {\"answer\": \"The shake-it-in-a-jar method is no...      0.000000   \n",
       "\n",
       "   answer_relevancy  context_recall  llm_context_precision_with_reference  \n",
       "0           0.92057             1.0                                   1.0  \n",
       "1           0.92550             1.0                                   1.0  \n",
       "2           0.00000             1.0                                   0.7  \n",
       "3           0.00000             0.0                                   0.0  \n",
       "4           0.00000             0.0                                   0.0  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_result_df = eval_results.to_pandas()\n",
    "evaluation_result_df.iloc[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
