{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107c1db-f471-4ccc-8fad-e6fbe08d0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_solution.create_db import create_db\n",
    "\n",
    "db = create_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27daf9d1-34ee-4a79-9af5-b556aadc6958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.retrieve(\"How do you pick a green?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0362f7-abc1-4043-9ba8-a3648696e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input and response handling\n",
    "from chat_solution.rag import QuizRag\n",
    "\n",
    "query1 = \"what is up?\"\n",
    "query2 = \"How do you pick a green?\"\n",
    "rag = QuizRag()  \n",
    "response = rag.query(query2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c60285d-241a-4027-b7f2-3f27a58e5e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 chunks of size 700 with overlap 200\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Convert each text chunk to a LangChain Document\n",
    "from langchain.schema import Document\n",
    "from chat_solution.create_db import create_text_chunks_from_workshop_data\n",
    "\n",
    "text_chunks = create_text_chunks_from_workshop_data()\n",
    "langchain_docs = [\n",
    "    Document(page_content=text, metadata={\"source\": f\"chunk_{i+1}\"})\n",
    "    for i, text in enumerate(text_chunks)\n",
    "]\n",
    "\n",
    "print(len(langchain_docs))\n",
    "# Display documents with metadata\n",
    "#for doc in langchain_docs:\n",
    "#    print(doc.page_content, doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0000fbcc-8f12-4e14-bc70-642937d55ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatMistralAI(model=\"mistral-large-latest\"))\n",
    "metrics = [LLMContextRecall(), FactualCorrectness(), Faithfulness()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75feb97b-598c-4a2a-aab5-e4fd91b96a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatMistralAI(model=\"mistral-large-latest\"))\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c1a3fc0-7d01-4dc7-b340-f9b3c6b63fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  50%|█████     | 6/12 [00:43<00:34,  5.79s/it]Property 'themes' already exists in node '2eb6d9'. Skipping!\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  58%|█████▊    | 7/12 [00:44<00:22,  4.49s/it]Property 'themes' already exists in node '8af9d7'. Skipping!\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  67%|██████▋   | 8/12 [00:48<00:17,  4.33s/it]Property 'themes' already exists in node '3f5c71'. Skipping!\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  75%|███████▌  | 9/12 [01:03<00:23,  7.69s/it]"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm)\n",
    "dataset = generator.generate_with_langchain_docs(langchain_docs,transforms_embedding_model=generator_embeddings, testset_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1856cab-65a9-4212-95d5-a42725294ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepared questions dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"atitaarora/food_lab_green_qna\", split=\"train\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcafb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ee129-023b-4942-bb7d-8717baf88432",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample question data\n",
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca66ec5-6486-4956-a361-5b0fed11145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ragas import evaluate\n",
    "#results = evaluate(dataset=dataset, metrics=metrics, llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0cfa60f6-ee2d-4cc8-821c-07b7fddd745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparation of Eval dataset for RAGAS (https://docs.ragas.io/en/stable/concepts/components/eval_sample/?h=singleturnsample#example)\n",
    "##for ragas dataset needs to be in the designated format \n",
    "from ragas import EvaluationDataset, SingleTurnSample\n",
    "from ragas.metrics import Faithfulness\n",
    "from datasets import load_dataset\n",
    "from ragas import evaluate\n",
    "import time\n",
    "\n",
    "samples = []\n",
    "eval_size = 5\n",
    "\n",
    "for i in range(eval_size):\n",
    "    entry = dataset[i]\n",
    "    \n",
    "    # Perform the query with a delay to limit to 1 request per second\n",
    "    user_query = entry['query']\n",
    "    response = rag.query(user_query)\n",
    "    \n",
    "    sample = SingleTurnSample(\n",
    "        user_input=user_query,\n",
    "        reference=entry['reference_answer'],\n",
    "        response=response,\n",
    "        retrieved_contexts=db.retrieve(user_query),\n",
    "    )\n",
    "    samples.append(sample)\n",
    "    \n",
    "    # Wait for 1-2 second before proceeding to the next iteration as we are limited by Mistral API\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5fadb-0058-4a87-9b13-bdbf23e5b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(samples)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfefec-a59a-4a29-9cd9-d6372dbe1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actual Evaluation\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "from ragas.metrics import NonLLMContextRecall\n",
    "from ragas.metrics import LLMContextRecall\n",
    "from ragas.metrics import Faithfulness\n",
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "eval_dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "faithfulness = Faithfulness()\n",
    "context_precision = LLMContextPrecisionWithReference()\n",
    "context_recall = NonLLMContextRecall()\n",
    "llm_context_recall = LLMContextRecall()\n",
    "answer_relevancy = ResponseRelevancy()\n",
    "\n",
    "eval_results = evaluate(\n",
    "        dataset=eval_dataset,\n",
    "        metrics=[\n",
    "                faithfulness,\n",
    "                answer_relevancy,\n",
    "                #context_recall, #This metric [non_llm_context_recall] that is used requires the following additional columns ['reference_contexts'] to be present in the dataset.\n",
    "                llm_context_recall,\n",
    "                context_precision,\n",
    "        ],\n",
    "       #llm=evaluator_llm\n",
    "       raise_exceptions=False \n",
    "    )\n",
    "#eval_results = evaluate(\n",
    "#    dataset=eval_dataset,\n",
    "#    metrics=[metric],\n",
    "#llm=evaluator_llm\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c140c6-443b-4e1f-919e-afb9954d6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result_df = eval_results.to_pandas()\n",
    "evaluation_result_df.iloc[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
