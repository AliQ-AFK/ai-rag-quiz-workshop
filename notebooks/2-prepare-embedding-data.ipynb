{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Embeddings Work Under the Hood\n",
    "\n",
    "## 1. The Journey from Text to Numbers\n",
    "\n",
    "Embeddings transform text into numbers through several steps:\n",
    "\n",
    "1. **Tokenization**: Breaking text into smaller pieces\n",
    "   - Words: \"hello world\" → [\"hello\", \"world\"]\n",
    "   - Subwords: \"playing\" → [\"play\", \"##ing\"]\n",
    "   - Characters: For languages like Chinese\n",
    "\n",
    "2. **Token IDs**: Each token gets mapped to a number\n",
    "   - Example: \"hello\" → 15234\n",
    "   - These mappings are stored in the model's vocabulary\n",
    "\n",
    "3. **Neural Network Processing**:\n",
    "   - Tokens pass through multiple transformer layers\n",
    "   - Each layer learns different aspects (syntax, semantics, context)\n",
    "   - Final layer outputs the embedding vector\n",
    "\n",
    "4. **Vector Space**: The final embedding places similar meanings close together\n",
    "   - Each dimension captures different semantic features\n",
    "   - Typical sizes: 384, 768, or 1024 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "text = \"I love machine learning\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# 2. Getting Embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding = model.encode(text)\n",
    "\n",
    "print(\"\\nEmbedding shape:\", embedding.shape)\n",
    "print(\"First 5 dimensions:\", embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Embedding Dimensions\n",
    "\n",
    "Each dimension in an embedding vector captures different semantic features:\n",
    "- Word types (noun, verb, etc.)\n",
    "- Topics (technology, nature, etc.)\n",
    "- Sentiment (positive, negative)\n",
    "- And many other abstract features\n",
    "\n",
    "The more dimensions, the more nuanced the representation, but also:\n",
    "- More computational cost\n",
    "- More storage needed\n",
    "- Risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how embeddings group similar concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create word groups with clear relationships\n",
    "words = {\n",
    "    'food': ['pizza', 'burger', 'pasta', 'sushi'],\n",
    "    'drinks': ['coffee', 'tea', 'juice', 'water'],\n",
    "    'colors': ['red', 'blue', 'green', 'yellow']\n",
    "}\n",
    "\n",
    "# Get embeddings for all words\n",
    "all_words = [word for group in words.values() for word in group]\n",
    "embeddings = model.encode(all_words)\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot with different colors for each group\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#FF9999', '#66B2FF', '#99FF99']\n",
    "for (group_name, group_words), color in zip(words.items(), colors):\n",
    "    start_idx = all_words.index(group_words[0])\n",
    "    x = embeddings_2d[start_idx:start_idx+4, 0]\n",
    "    y = embeddings_2d[start_idx:start_idx+4, 1]\n",
    "    plt.scatter(x, y, c=color, label=group_name)\n",
    "    \n",
    "    # Add word labels\n",
    "    for i, word in enumerate(group_words):\n",
    "        plt.annotate(word, (x[i], y[i]))\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "plt.title(\"Word Embeddings Visualized in 2D\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Show some similarity scores\n",
    "print(\"\\nSimilarity Scores (closer to 1 = more similar):\")\n",
    "print(f\"pizza-burger: {cosine_similarity(model.encode('pizza'), model.encode('burger')):.3f}\")\n",
    "print(f\"coffee-tea: {cosine_similarity(model.encode('coffee'), model.encode('tea')):.3f}\")\n",
    "print(f\"pizza-coffee: {cosine_similarity(model.encode('pizza'), model.encode('coffee')):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why Different Models for Different Languages?\n",
    "\n",
    "Language-specific models perform better because:\n",
    "\n",
    "1. **Vocabulary Coverage**: \n",
    "   - Better handling of language-specific words\n",
    "   - Proper subword tokenization for morphologically rich languages\n",
    "\n",
    "2. **Cultural Context**:\n",
    "   - Understanding idioms and expressions\n",
    "   - Proper handling of formal/informal speech\n",
    "\n",
    "3. **Syntactic Structure**:\n",
    "   - Word order differences\n",
    "   - Grammar patterns\n",
    "\n",
    "This is why we use different models in our code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Tips for Using Embeddings\n",
    "\n",
    "1. **Choosing Model Size**:\n",
    "   - Smaller models (384 dims): Faster, good for simple tasks\n",
    "   - Larger models (768+ dims): Better quality, slower\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - Clean text (remove noise)\n",
    "   - Consistent casing\n",
    "   - Handle special characters\n",
    "\n",
    "3. **Storage Considerations**:\n",
    "   - 384 dimensions × 4 bytes = ~1.5KB per embedding\n",
    "   - Plan database capacity accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Embeddings in Language Models\n",
    "\n",
    "Embeddings are numerical representations of text that capture semantic meaning. They convert words or sentences into vectors (lists of numbers) that can be compared mathematically.\n",
    "\n",
    "Key points about embeddings:\n",
    "- Similar texts should have similar vector representations\n",
    "- The dimensionality and quality of embeddings affects performance\n",
    "- Different embedding models are trained on different data\n",
    "\n",
    "Let's explore this with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load different embedding models\n",
    "english_model = SentenceTransformer('all-MiniLM-L6-v2')  # English-focused\n",
    "german_model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True) # Multi-Lingual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Comparing Similar Sentences\n",
    "\n",
    "Let's see how embeddings capture similarity between sentences in different languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def compare_sentences(model, sent1, sent2):\n",
    "    emb1 = model.encode(sent1)\n",
    "    emb2 = model.encode(sent2)\n",
    "    return cosine_similarity(emb1, emb2)\n",
    "\n",
    "# Test pairs in different languages\n",
    "english_pairs = [\n",
    "    (\"I love Berlin\", \"Berlin is my favorite city\"),\n",
    "    (\"I love Berlin\", \"I hate vegetables\")\n",
    "]\n",
    "\n",
    "german_pairs = [\n",
    "    (\"Ich liebe Berlin\", \"Berlin ist meine Lieblingsstadt\"),\n",
    "    (\"Ich liebe Berlin\", \"Ich hasse Gemüse\")\n",
    "]\n",
    "\n",
    "print(\"\\nEnglish Model Results:\")\n",
    "for sent1, sent2 in english_pairs:\n",
    "    sim = compare_sentences(english_model, sent1, sent2)\n",
    "    print(f\"{sent1} <-> {sent2}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nGerman-English Model Results:\")\n",
    "for sent1, sent2 in german_pairs:\n",
    "    sim = compare_sentences(german_model, sent1, sent2)\n",
    "    print(f\"{sent1} <-> {sent2}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Cross-lingual Capabilities\n",
    "\n",
    "Let's compare how different models handle cross-lingual similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_lingual_pairs = [\n",
    "    (\"The weather is nice today\", \"Das Wetter ist heute schön\"),\n",
    "    (\"I need a coffee\", \"Ich brauche einen Kaffee\"),\n",
    "    (\"Berlin is the capital of Germany\", \"Berlin ist die Hauptstadt von Deutschland\")\n",
    "]\n",
    "\n",
    "print(\"Cross-lingual similarity:\")\n",
    "print(\"\\nEnglish-only model:\")\n",
    "for en, de in cross_lingual_pairs:\n",
    "    sim = compare_sentences(english_model, en, de)\n",
    "    print(f\"{en} <-> {de}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nGerman-English model:\")\n",
    "for en, de in cross_lingual_pairs:\n",
    "    sim = compare_sentences(german_model, en, de)\n",
    "    print(f\"{en} <-> {de}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Domain-Specific Comparisons\n",
    "\n",
    "Let's see how models handle domain-specific terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_pairs = [\n",
    "    (\"Guided tour of the Brandenburg Gate\", \"Führung durch das Brandenburger Tor\"),\n",
    "    (\"Skip the line tickets for museums\", \"Eintrittskarten ohne Anstehen für Museen\"),\n",
    "    (\"Best restaurants in Berlin\", \"Beste Restaurants in Berlin\")\n",
    "]\n",
    "\n",
    "print(\"Tourism domain comparisons:\")\n",
    "for en, de in tourism_pairs:\n",
    "    en_sim = compare_sentences(english_model, en, de)\n",
    "    de_sim = compare_sentences(german_model, en, de)\n",
    "    print(f\"\\nPair: {en} <-> {de}\")\n",
    "    print(f\"English model similarity: {en_sim:.3f}\")\n",
    "    print(f\"German-English model similarity: {de_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Language Support**: Models trained on specific languages perform better for those languages\n",
    "2. **Cross-lingual Capabilities**: Specialized multilingual models handle cross-language comparisons better\n",
    "3. **Domain Relevance**: Consider your use case when choosing an embedding model\n",
    "\n",
    "When building a RAG system:\n",
    "- Choose embedding models that match your content languages\n",
    "- Consider using specialized models for specific domains\n",
    "- Test different models with your actual use cases (And evaluate those)\n",
    "- Balance performance vs computational cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
